---
layout: about
title: About
permalink: /
subtitle:

profile:
  align: center
  image: photo.png
  image_circular: false # crops the image to make it circular
  address:

news: false  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am a Senior Research Scientist at [Yandex Research](https://research.yandex.com/) and a Ph.D. student (graduating in mid-2023) at the [Faculty of Computer Science](https://cs.hse.ru/en/) of [HSE University](https://www.hse.ru/en/) advised by Artem Babenko.
At the [BigScience Workshop](https://bigscience.huggingface.co/), I served as the chair of the Engineering and Scaling working group.
Earlier, I was a Machine Learning intern at [Replika](https://replika.ai/) and [Yandex Translate](https://translate.yandex.com/).
During my undergraduate studies, I was a member of the [Bayesian Methods Research Group](https://bayesgroup.ru/) and worked under the supervision of [Ekaterina Lobacheva](https://scholar.google.com/citations?user=8D4Be1sAAAAJ&hl=en).

Besides my research work, I also teach [Efficient Deep Learning Systems](https://github.com/mryab/efficient-dl-systems) at HSE University and [Yandex School of Data Analysis](https://academy.yandex.com/dataschool) (all materials are in English) and [Deep Learning](https://github.com/mryab/dl-hse-ami) at HSE University (mostly in Russian).

I am mainly interested in solving new problems with Natural Language Processing models and making current advances in deep learning more efficient and broadly accessible. In particular, my research on decentralized deep learning has served as a basis of [Hivemind](https://github.com/learning-at-home/hivemind), a PyTorch library for training neural networks over heterogeneous, unreliable and poorly connected hardware.
Before that, I also worked on topics like uncertainty estimation in machine translation, graph-based word representations, neural networks with adaptive computation, and gradient optimization of decoding hyperparameters in language generation.

If you have any questions about my work, have ideas for collaboration or simply want to say hi, feel free to drop me a line using any of the links below!
